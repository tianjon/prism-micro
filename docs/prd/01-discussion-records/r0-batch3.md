# R0 独立立场宣言 — Batch 3

> **专家**：陈思琪（AI/ML 工程师）、王磊（全栈工程师）
> **生成时间**：2026-02-12
> **规则**：两位专家独立撰写，互不参考对方立场

---

## 陈思琪 · AI/ML 工程师 — "AI 边界的布道者"

---

### 陈思琪 · a 第一阶段的边界在哪里？

**立场**：有条件支持（产品第一阶段应推进到技术 Phase 3）

**一句话观点**：产品第一阶段的边界必须到 Phase 3，因为只有四阶段 AI 管线落地才能证明 Prism 不是又一个 CRUD。

**关键论据**：

1. 根据路线图设计（`07-roadmap.md` Phase 3 核心交付物），Phase 3 是 Prism 从"通用 LLM 网关"蜕变为"VOC 分析平台"的关键阶段——只有四阶段 AI 管线（拆解 → 标签涌现 → 向量化 → 关系构建）完整落地后，系统才具备语义理解、涌现标签、向量检索这三个核心 AI 能力。如果产品第一阶段止步于 Phase 2 的 LLM 调用能力，用户看到的仅仅是一个 Chat/Embedding API 网关——这和 OpenRouter、One API 等开源方案没有本质区别，VP1（涌现式标签 vs 预设分类）完全无法体现。

2. 资源与 ROI 分析（`09-resource-roi.md` 第 5.3 节退出策略）明确指出"Phase 3 是价值拐点"，累计投入约 112 万，获得可工作的 VOC 分析核心能力。这意味着路线图的设计者自己也认同——没有到 Phase 3，Prism 不构成一个"产品"。AI 管线的四阶段处理是将非结构化文本转化为语义知识的核心流程，每阶段都有不同的 AI 依赖（Stage 1/2 依赖 LLM Chat、Stage 3 依赖 Embedding API），这些依赖恰好在 Phase 2 的 LLM 调用能力基础上运行。Phase 2 是为 Phase 3 铺路的，而非终点。

3. 从技术验证角度看，AI 管线中三个关键假设——LLM 语义拆解的质量、涌现标签的有效性、pgvector 在 10 万向量内的性能（`09-resource-roi.md` 第 5.4 节技术验证节点）——都必须在 Phase 3 被验证。如果产品第一阶段不包含 Phase 3，这些技术假设将长期悬而未决，团队无法确认核心技术方案是否可行。越晚验证，调整方向的代价越高——AI 管线的 Prompt 工程、标签标准化流水线、向量索引参数都需要在真实数据上迭代调优。

**最大风险**：Phase 1 + 2 + 2.5 + 3 的累计工期为 18-24 周，对 4.5 人团队而言是一个较长的纯投入期。如果中途遇到 LLM 语义拆解质量不达标或涌现标签效果在小数据量下不显著，可能导致项目延期且难以向管理层展示中间价值。

---

### 陈思琪 · b MVP 功能范围多大？

**立场**：支持（包含数据摄入 + 涌现标签 + 语义搜索）

**一句话观点**：MVP 至少要跑通"数据进 → AI 处理 → 语义搜索出结果"的完整链路，否则不配叫 MVP。

**关键论据**：

1. AI 摄入管线（`03-ingestion-pipeline.md`）定义了四阶段处理流程：语义拆解 → 标签涌现 → 向量化 → 关系构建。我认为 MVP 至少需要完成前三个阶段。Stage 1（语义拆解）将一条 Voice 拆为多个 SemanticUnit，每个 Unit 携带意图、情感、置信度；Stage 2（标签涌现）为每个 Unit 生成 3-7 个自由形式标签；Stage 3（向量化）将 Unit 转为语义向量存入 pgvector。这三个阶段合在一起，才能让用户通过 `vector_search` 搜索"支付卡顿"时，得到语义相关（而非关键词匹配）的结果。Stage 4（关系构建）可以推迟——它主要提供 Unit 间的显式关系网络，是锦上添花而非核心功能。

2. 五大能力群（`04-core-capabilities.md` 第 2.3 节）明确提出涌现式标签的双轨设计是 Prism 的核心差异化：涌现轨捕获"你不知道你不知道的"新概念，预设轨保证统计口径一致。没有涌现标签，Prism 的标签系统就和传统的预设分类没有区别。文档中用数据论证——关键词召回率约 30%，语义召回率可达 85%+。MVP 如果只有 CRUD + Chat API 而没有涌现标签和语义搜索，用户无法感受到这种量级差异。

3. 成本可控：根据摄入管线的成本估算（`03-ingestion-pipeline.md` 成本估算部分），单条 Voice 的端到端处理成本约 $0.0035（约 0.028 元），1000 条 Voice 约 $3.5。MVP 阶段的数据量不大（几百到几千条），LLM API 成本完全可控。Embedding 模型推荐使用 BGE-large-zh-v1.5（在 `07-roadmap.md` Phase 3 关键决策中明确），该模型在中文语义理解 benchmark 上表现最优，1024 维向量通过 LLM 网关的别名系统调用，不需要额外部署模型服务。

**最大风险**：涌现标签在数据量很小（如 100 条以内）时，标签标准化的同义词映射表尚未积累足够条目，标签可能呈现"散乱"状态——"加载慢""响应慢""卡顿""很慢""太慢了"可能同时存在而未被自动合并。这可能让早期用户觉得标签系统"不智能"。

---

### 陈思琪 · c Agent-First 从第一天就要吗？

**立场**：支持

**一句话观点**：Agent 骨架必须从第一天搭建，否则后续改造成本是初始的 5-10 倍，这是 Type 1 决策。

**关键论据**：

1. Agent-First 设计哲学（`05-agent-first-design.md` 第 1.3 节）用详尽的分析证明了"先不管 Agent，以后再加"的真实成本：需要重写认证系统（从 JWT-only 扩展到 API Key + 统一 Principal）、重写 API 层（从"面向页面"重构为"面向能力"的原子接口）、重写权限模型（从粗粒度角色到 Capability 白名单）、重写审计日志、重建数据接口。这五项改造相互依赖——改认证必须同时改权限，改权限必须同时改审计。引用文档原文："架构级改造的成本是初始设计成本的 5-10 倍"。Prism 的 Skill 体系（JSON Schema 声明式定义、输入输出契约、权限声明、成本元数据）必须在 Phase 2.5 就设计到位。

2. 从路线图依赖关系看（`07-roadmap.md` 第 9 节），Phase 3 是唯一同时依赖 Phase 2 和 Phase 2.5 的阶段——它既需要 LLM 网关的 Chat/Embedding API 完成四阶段管线处理，又需要 Agent 运行时的 Skill 注册表来安置 8 个原子查询工具（vector_search、get_neighbors、random_sample 等）。如果 Phase 2.5 不先于 Phase 3 完成，Phase 3 的 8 个原子查询工具就只能以"裸 API"形式交付，后续再包装成 Skill 塞进 Agent 框架，就是典型的"后期改造"。路线图文档估算这会多出 3-4 周的集成和调试时间。

3. 双身份认证（Human JWT + Agent API Key → 统一 Principal）在 `05-agent-first-design.md` 第 2.1 节被定义为"Type 1（不可逆）决策"。一旦系统在没有 Agent 身份模型的情况下积累了用户数据和 API 调用链，后续引入 Agent 身份就需要迁移所有现有的认证路径和审计日志格式。这不是"加一个新功能"，而是"改变系统的身份基座"。Phase 2.5 的投入（约 4-6 周、2 名后端 + 0.5 名 AI 工程师）是一笔划算的"保险费"。

**最大风险**：Phase 2.5 的 Agent 运行时在交付时只有 llm_chat 这一个可用 Skill，Agent 能力非常有限（只能做简单 LLM 对话）。管理层可能质疑"花了 4-6 周搭了一个只能聊天的 Agent，价值在哪里？"需要有效沟通 Phase 2.5 的战略意义在于"骨架就位"而非"当前能力"。

---

### 陈思琪 · d 涌现标签是否必须进入第一阶段？

**立场**：支持

**一句话观点**：涌现标签是 Prism 的产品灵魂——VP1 的核心载体，没有它就是又一个 CRUD + GPT Wrapper。

**关键论据**：

1. 五大能力群深度剖析（`04-core-capabilities.md` 第 2.3 节）详细论证了涌现式标签双轨设计的不可替代性：涌现轨让 LLM 自由生成标签（"搜索结果加载慢""iOS 17.2 指纹识别失败"等任意粒度），捕获传统预设分类永远无法覆盖的长尾概念。文档用具体数据说话——传统关键词召回率约 30%，涌现式语义标签覆盖率可达 85%+，这意味着 55 个百分点的召回率差距。ROI 分析（`09-resource-roi.md` 第 4.2 节定量 ROI 表格）也将"标签覆盖率从 ~30% 提升到 85%+"列为核心价值差异。如果第一阶段没有涌现标签，Prism 在标签维度上和任何一个基于预设分类的传统 VOC 工具没有区别。

2. 标签标准化是一个需要数据积累的过程。AI 管线设计（`03-ingestion-pipeline.md` Stage 2 标签涌现部分）详细描述了标签标准化的三道工序：文本清洗 → 同义词映射 → 向量相似度合并（相似度 > 0.95 自动建议合并）。同义词映射表和向量空间的标签拓扑需要随数据量增长而完善——这就是 `04-core-capabilities.md` 第 4.4 节描述的"标签飞轮"效应。越早开始积累标签数据，飞轮转得越快。如果把涌现标签推迟到后续阶段，不仅损失了积累时间，还要在已有数据上重新跑标签生成——既浪费 LLM API 成本，也可能因为回溯处理与实时处理的一致性问题引入 bug。

3. 涌现标签的技术实现成本并不高。Stage 2 的核心逻辑是一个 LLM Prompt 调用 + 标签标准化流水线。Prompt 设计已经在 `03-ingestion-pipeline.md` 中给出（要求 LLM 生成 3-7 个标签，标注相关度和是否为主标签）。标签标准化包括 `normalize_tag_name` 文本清洗和 `get_or_create_tag` 幂等创建。从工程量看，Stage 2 是四阶段管线中最轻量的——它不需要额外的基础设施（向量化需要 pgvector，关系构建需要相似度计算），只需要 LLM Chat API（Phase 2 已交付）加上一张标签表和标准化逻辑。

**最大风险**：涌现标签的效果在小数据量下可能不显著。当系统只有 100 条 Voice 时，涌现出的标签可能数量太少、覆盖面太窄，无法展现"涌现式发现未知"的魅力。用户可能看到的只是一堆零散的标签，而不是一个有组织的知识结构。需要设计好"冷启动"策略，比如提供种子数据或者演示数据集来展示涌现标签的潜力。

---

### 陈思琪 · e 前端投入多少？

**立场**：有条件支持（最小但关键的前端投入）

**一句话观点**：前端不需要精美，但必须让用户"看到"AI 的处理结果——数据浏览 + 标签可视化 + 语义搜索框。

**关键论据**：

1. 涌现标签和语义搜索的价值必须通过可交互的 UI 才能被感知。路线图（`07-roadmap.md` Phase 3 团队配置）为 Phase 3 分配了"前端 x1（数据浏览 UI）"的人力。这个分配是合理的——四阶段 AI 管线的处理结果（SemanticUnit 列表、涌现标签、向量检索结果）如果只能通过 curl 或 CLI 查看，非技术用户（产品经理、运营人员）根本无法评估系统价值。最小的前端需求应该包括：(1) 数据导入界面（CSV 上传）；(2) SemanticUnit 浏览列表（显示原文、摘要、意图、情感、标签）；(3) 语义搜索框（输入自然语言，返回语义相关的 Unit）；(4) 标签云或标签列表（展示涌现标签的统计分布）。

2. 用户体验层的三层渐进式信息架构（`04-core-capabilities.md` 第 6.1 节）是一个好的设计目标，但第一阶段不需要全部实现。第一阶段只需要"探索层"的最小子集——让用户能浏览数据、搜索、查看标签即可。"概览层"（CEO 仪表板）和"深钻层"（完整的溯源链路）可以推迟。前端的核心价值在于让 AI 处理结果"可见"，而不在于"美观"。

3. API First 原则（`07-roadmap.md` 第 1.3 节）确保了前端只是 API 的消费者之一。Phase 3 的 8 个原子查询工具先以 API 形式交付，前端调用这些 API 展示结果。这意味着前端开发不在关键路径上——API 做好后，前端可以并行甚至延后开发。但"延后"不等于"不做"，至少需要一个最基本的数据浏览和搜索界面来支持 dogfooding 和演示。

**最大风险**：前端投入如果控制不好，容易范围蔓延。"加个筛选条件""改个列表排序""加个图表"——每个小需求单独看都不大，累积起来可能吞噬前端工程师的全部时间。需要严格的功能冻结纪律，第一阶段的前端只做"数据浏览 + 搜索 + 标签展示"三件事。

---

### 陈思琪 · f 目标用户是谁？第一阶段给谁用？

**立场**：有条件支持（先 dogfooding，但必须用 AI 管线处理真实数据）

**一句话观点**：先内部 dogfooding，但必须导入真实的用户反馈数据跑完 AI 管线，而非用 mock 数据自嗨。

**关键论据**：

1. AI 管线的质量验证需要真实数据。根据 `09-resource-roi.md` 第 5.4 节技术验证节点，"涌现式标签质量"的验证方式是"人工评审覆盖率 + 一致性指标"。这意味着我们需要用真实的用户反馈（而非编造的测试数据）跑完四阶段管线，然后让内部团队评审 LLM 拆解是否准确、涌现标签是否有意义、语义搜索是否返回了相关结果。Mock 数据无法暴露 LLM 在真实场景下的边界情况——比如混合语言、口语化表达、带情绪的长文本。内部 dogfooding 的最大价值不是"验证产品需求"，而是"验证 AI 管线在真实数据上的表现"。

2. BGE-large-zh-v1.5 模型在中文语义任务上的 benchmark 表现优异（路线图 Phase 3 关键决策提及该模型为中文语义理解最优，1024 维向量），但 benchmark 性能和实际业务数据上的表现之间可能存在 gap。我们的 VOC 数据可能包含行业专有术语、缩写、emoji、中英文混排等，这些在标准 benchmark 中覆盖不足。只有用真实数据 dogfooding 才能发现这些问题，并针对性地调优 Prompt 和标签标准化规则。

3. 同时，我认为 dogfooding 阶段应该有计划地记录 AI 管线的表现指标：SemanticUnit 拆解准确率、标签相关度、语义搜索 Top-5 命中率等。这些指标是未来面向外部用户时的"信心基础"。如果 dogfooding 阶段的数据证明涌现标签覆盖率确实显著优于预设分类，这就是最有力的产品说服力。

**最大风险**：团队成员不是目标用户（VOC 分析师或产品经理），他们对 AI 管线处理结果的评判标准可能与真实用户不同。技术团队可能过于关注"AI 是否正确"，而忽略了"这个结果是否对产品决策有帮助"。

---

### 陈思琪 · g 第一阶段是否需要包含治理/采纳机制？

**立场**：有条件支持（最小的 AI 质量反馈通道，但不是完整治理）

**一句话观点**：需要"标签有用/无用"的最简反馈机制作为 AI 管线质量闭环，但完整治理推迟到 Phase 4。

**关键论据**：

1. LLM 输出守卫层（`04-core-capabilities.md` 第 2.4 节三级降级策略）解决的是 AI 输出的"格式正确性"问题（L1 正常 → L2 修正 → L3 降级），但不解决"语义正确性"问题。一个格式完美但标签错误的结果会通过守卫层进入知识库。因此，最小的人工反馈通道是必要的——至少让用户能标记"这个标签不准确"或"这两个标签应该合并"。这不需要 Phase 4 的完整 Concept 治理工作台（五个治理操作 + 审计轨迹），只需要一个轻量的"标签反馈"接口。

2. 标签标准化流水线的同义词映射表（`03-ingestion-pipeline.md` 标签标准化部分）需要人工种子数据来冷启动。LLM 不同调用之间可能为相同概念生成不同标签（"加载慢""响应慢""卡顿"），向量相似度合并（阈值 0.95）可以捕获一部分，但有些同义词在向量空间中的距离可能 > 0.05（比如"闪退"和"App 崩溃"）。一个简单的"这两个标签是同一件事"的合并按钮，就能帮助系统快速积累同义词知识。

3. 但我反对在第一阶段引入完整的 Signal → Concept 治理流程。Phase 4 的 Concept 治理（`04-core-capabilities.md` 第 4.2 节）包含确认、命名、合并、静音、追踪五个操作，以及全操作审计轨迹、五维优先级评估器、三频率 Signal 自动产生——这些是相当复杂的系统设计，依赖 Phase 3 的向量检索和标签统计基础设施。在第一阶段强行引入完整治理，工期不允许，也没有足够的数据量让治理机制体现价值。

**最大风险**：即使是"最小反馈通道"也需要前后端开发——后端需要反馈记录 API + 标签合并 API，前端需要在标签展示旁边加反馈按钮。如果对"最小"的定义把控不好，可能滑坡成一个小型治理系统，挤占 AI 管线核心开发的时间。

---

## 王磊 · 全栈工程师 — "务实到极致的交付战士"

---

### 王磊 · a 第一阶段的边界在哪里？

**立场**：有条件支持（边界应在团队 4-6 周可交付的范围内动态确定）

**一句话观点**：别讨论边界应该"在哪里"，先看团队当前速度能"走到哪里"，4-6 周能交什么就交什么。

**关键论据**：

1. Phase 1 已完成、Phase 2 进行中——这是当前事实（`phase1-deliverables.md` + `09-resource-roi.md` 第 3.3 节当前进展）。Phase 1 交付了 shared 共享库、user-service 认证、llm-service 管理功能、Web UI 和 Docker Compose 基础设施，全部通过验收。Phase 2 的 LiteLLM 集成、Chat/Embedding API、故障转移引擎正在开发中。这些都是已经落地的代码，不需要再讨论。"第一阶段的边界"这个问题，实际上是在问"Phase 2 完成后，接下来 4-6 周我们还能做多少"。

2. 从工时估算角度看（`09-resource-roi.md` 第 3.1 节各阶段时间线），Phase 2.5（Agent 运行时）预估 4-6 周，Phase 3（VOC 数据摄入 + 语义底座）预估 6-8 周。两个阶段合计 10-14 周。如果产品第一阶段要到 Phase 3 结束，那就是在告诉管理层"再等 3-4 个月才能看到核心功能"。我认为更务实的做法是：Phase 2 完成后，用 4-6 周做一个"Phase 2.5 精简版 + Phase 3 核心子集"，先让系统能导入数据、跑 AI 管线、做基本搜索。

3. 路线图说"每个阶段独立可交付、可演示"（`07-roadmap.md` 第 1.1 节）。我完全同意这个原则，但要把它应用到实践中：每 4-6 周必须有可演示的增量交付。如果某个"阶段"需要 6-8 周才能看到东西，那它的粒度太粗了。我建议将 Phase 3 拆为至少 2 个子阶段——Phase 3a（数据导入 + Stage 1 语义拆解 + 基本浏览）和 Phase 3b（Stage 2 标签涌现 + Stage 3 向量化 + 语义搜索），每个子阶段 3-4 周可交付。

**最大风险**：过度切分阶段可能导致每个子阶段的交付物太单薄，不足以展现系统价值。"能导入 CSV 但只能看到拆解后的文本"可能不如"什么都做完后一次性展示"有冲击力。

---

### 王磊 · b MVP 功能范围多大？

**立场**：有条件支持（严格按 Must / Should / Won't 分级）

**一句话观点**：MVP 必须可工作、可演示，但范围必须砍到团队 6 周内能交付的最小集。

**关键论据**：

1. 我按工时估算对 MVP 功能做分级。以下基于当前团队（后端 x2 + 前端 x0.5-1 + AI 工程师 x0.5-1）的实际产能估算：

   **Must Have（6 周内必须完成）**：
   - CSV 数据导入接口（2 人天）
   - Stage 1 语义拆解 + LLM Prompt（5 人天）
   - Stage 2 标签涌现 + 标签标准化基础版（5 人天）
   - Stage 3 向量化 + pgvector 存储（3 人天）
   - `vector_search` 语义搜索 API（3 人天）
   - 基本数据浏览 Web UI（5 人天）
   - LLM 输出守卫层 L1/L2（3 人天）
   - 以上合计约 26 人天，2 名后端 + 1 名 AI 工程师的 6 周产能约 90 人天，留有余量

   **Should Have（时间允许再做）**：
   - Stage 4 关系构建（5 人天）
   - `get_tags` + `get_units_by_tag` 查询 API（4 人天）
   - 标签云可视化（3 人天）
   - L3 降级策略（2 人天）

   **Won't Have（第一阶段不做）**：
   - 完整的 8 个原子查询 Skill（Phase 3 后续完善）
   - Skill 注册表 + Agent Loop 整合（Phase 2.5 范畴）
   - Source Adapter 声明式框架（先用最简的 CSV 导入）
   - 前端数据可视化图表（标签趋势、情感分布等）

2. Phase 1 交付标准（`phase1-deliverables.md`）给出了一个好的参考——它定义了"必须交付"和"不在第一期范围"两个清晰的列表。我建议沿用这个模式，对扩展后的产品第一阶段同样做出硬性的范围承诺。承诺做的，deadline 前交付；不承诺的，一行代码都不写。

3. 路线图中 Phase 3 的验收标准（`07-roadmap.md` Phase 3 验收标准）写了 6 条，其中第 1、2、3 条（CSV 导入 + 四阶段处理 + 语义搜索）是核心，第 4 条（Agent 组合 Skill 分析）依赖 Phase 2.5 的 Agent 运行时，第 5 条（性能指标）和第 6 条（L3 降级）可以作为 Should Have。

**最大风险**：Must Have 清单中的工时估算可能偏乐观——LLM Prompt 的调优迭代、标签标准化的边界情况处理、pgvector 索引参数调优都可能比预期耗时更长。如果按我的估算严格执行，Should Have 的内容可能全部被砍掉。

---

### 王磊 · c Agent-First 从第一天就要吗？

**立场**：反对（Agent 基础设施可以"够用就行"，不需要从第一天一步到位）

**一句话观点**：Agent Loop 先做最简版，双身份认证用中间件 hack，Skill 注册表用配置文件代替数据库。

**关键论据**：

1. Phase 2.5 的完整交付物（`07-roadmap.md` Phase 2.5 核心交付物）包括 Skill 注册表（声明式定义 + CRUD API + 权限校验）、基础 Agent Loop（ReAct 循环）、双身份认证（统一 Principal）、执行上下文管理（权限边界 + 资源配额 + 审计日志）。这是一个 4-6 周的工作量，需要后端 x2 + 前端 x0.5。我的问题是：Phase 2.5 交付后，Agent 只有一个可用 Skill（llm_chat），能做的事只是"通过 API Key 认证后和 LLM 聊天"。这个投入产出比值得吗？

2. 我提出的替代方案是"渐进式 Agent 基础设施"：
   - **双身份认证**：Phase 2 的 user-service 已经有 JWT 认证。Agent API Key 可以先实现为一个简单的中间件——校验 API Key、注入一个 agent_id 到请求上下文。统一 Principal 抽象可以在 Phase 3 开始前再做。工时：3 人天 vs 完整方案的 10 人天。
   - **Skill 注册表**：先不做数据库持久化和 CRUD API，用一个 Python 配置文件（或 YAML）定义可用 Skill 列表。Phase 3 的 8 个原子查询工具直接注册在配置文件里。工时：2 人天 vs 完整方案的 8 人天。
   - **Agent Loop**：用最简的 while 循环 + LLM Function Calling 实现。不做成本追踪、不做迭代上限配置化、不做优雅终止——这些 Phase 3 交付后再补。工时：5 人天 vs 完整方案的 15 人天。

3. 我的方案总工时约 10 人天，完整 Phase 2.5 方案约 33 人天（参考团队配置：后端 x2 + 前端 x0.5，4-6 周）。差距是 3 倍。省下来的 23 人天可以用来提前启动 Phase 3 的 AI 管线开发，让用户更早看到核心功能。

**最大风险**：赵一凡说的对——如果简化版的 Agent 基础设施在后续需要大改，改造成本可能超过节省的 23 人天。特别是双身份认证如果在有数据积累后再改，确实存在迁移风险。但我认为在 Phase 3 之前（系统还没有真实用户数据），这个风险是可控的。

---

### 王磊 · d 涌现标签是否必须进入第一阶段？

**立场**：有条件支持（进入，但实现范围要严格控制）

**一句话观点**：涌现标签可以做，但只做 Stage 2 核心逻辑 + 基础标准化，不要碰同义词治理和向量合并。

**关键论据**：

1. 涌现标签的核心实现并不复杂——`03-ingestion-pipeline.md` 中 Stage 2 的处理逻辑清晰：一个 LLM Prompt 调用 + JSON 解析 + `normalize_tag_name` 文本清洗 + `get_or_create_tag` 幂等创建。我估算纯开发工时约 5 人天（含 Prompt 编写、标签表 schema、标准化函数、单元测试）。这个投入是可接受的。

2. 但我强烈反对在第一阶段做以下"看似标签相关但工时巨大"的功能：
   - **向量相似度合并**（`04-core-capabilities.md` 提到"为每个标签生成 embedding，相似度 > 0.95 自动建议合并"）：这需要为标签单独跑 Embedding、存储标签向量、实现相似度计算和合并建议 UI。工时估算 8-10 人天，投入产出比不高。
   - **同义词映射表维护 UI**：允许用户手动配置"闪退 = App 崩溃"的映射关系，需要前后端各 3-5 人天。
   - **标签统计仪表板**（使用次数、7 天/30 天趋势、平均情感倾向等）：需要定时任务 + 聚合查询 + 前端图表，工时 5-8 人天。

   以上三项合计 16-23 人天，几乎等于涌现标签核心实现的 3-4 倍。Must Have 是核心标签生成和基础标准化，其余全部是 Won't Have。

3. 周安担心的 LLM 幻觉污染知识库的风险是真实的，但我的应对方案不是"不上线涌现标签"，而是"所有标签标记置信度 + 降级标记"。AI 管线已经在 Stage 1 为每个 SemanticUnit 标记了 `confidence` 字段（`03-ingestion-pipeline.md` Prompt 设计部分），标签也标记了 `relevance` 分数。前端展示时加个"AI 生成"标识即可。工时：0.5 人天。

**最大风险**：基础标准化（只做 `normalize_tag_name` 文本清洗）可能不够——LLM 在不同调用中为同一概念生成的标签变体可能通过基础标准化后仍然是不同的条目（如"页面卡顿"和"页面加载缓慢"标准化后仍然不同）。这会导致标签表膨胀，用户看到一堆"差不多但不一样"的标签，影响体验。

---

### 王磊 · e 前端投入多少？

**立场**：有条件支持（最小化，复用 Phase 1 已有基础）

**一句话观点**：Phase 1 的 Web UI 已有基础，在此基础上加 3 个页面（数据导入、浏览列表、搜索框），不超过 10 人天。

**关键论据**：

1. Phase 1 交付的 Web UI（`phase1-deliverables.md` 第 3 项）已包含：登录页面、Provider 管理页面、Model 管理页面、Alias 管理页面。技术栈是 React 19 + Vite + shadcn/ui + Tailwind CSS（`07-roadmap.md` Phase 1 关键决策）。组件库和设计系统已经就位，新增页面的边际成本低。

2. 我的最小前端投入清单（Must / Should / Won't）：

   **Must Have（10 人天以内）**：
   - 数据导入页面：CSV 文件上传 + 导入进度展示（2 人天）
   - Voice/Unit 浏览列表：表格展示原文、摘要、意图、情感、标签（3 人天）
   - 语义搜索框：输入文本 → 调用 `vector_search` API → 展示结果列表（3 人天）
   - 标签列表页面：展示所有涌现标签 + 使用次数排序（2 人天）

   **Should Have**：
   - 搜索结果高亮和分页
   - Unit 详情弹窗（查看完整原文和上下文）
   - 标签点击筛选（点击标签查看关联 Unit）

   **Won't Have**：
   - Agent 对话界面（Phase 2.5/5 范畴）
   - 三层渐进式信息架构（概览层、探索层、深钻层）
   - 数据可视化图表（情感趋势线、标签热力图等）
   - Concept 治理工作台
   - SSE 流式对话

3. 路线图的 API First 原则（`07-roadmap.md` 第 1.3 节）明确说"如果时间紧张，可以延后 UI 的精细打磨，但不能延后 API 的设计和实现"。我完全同意。前端是 API 的消费者，API 做好了前端随时可以迭代。第一阶段前端只要"能用"就行，不需要"好看"。

**最大风险**：10 人天的前端投入可能做出来的 UI 比较粗糙（纯表格、无动画、布局简陋），给内部演示时第一印象不好。但我认为对于 dogfooding 阶段，功能可用比视觉精美重要得多。

---

### 王磊 · f 目标用户是谁？第一阶段给谁用？

**立场**：支持（先内部 dogfooding）

**一句话观点**：第一阶段给自己团队用，别花时间找 Design Partner——找人的时间不如写代码。

**关键论据**：

1. 当前团队规模是 ~2.5 人（`09-resource-roi.md` 第 1.2 节当前进展："当前 ~2.5 人"），到 Phase 3 峰值 5 人。这 4-5 个人的全部带宽应该花在写代码和验证技术假设上，不应该花在"找 Design Partner + 沟通需求 + 收集反馈 + 调整方向"的用户研究循环上。林晓薇要求的"找到至少一个 Design Partner"，在实操层面意味着：(1) 识别潜在合作企业（1-2 周）；(2) 沟通对接、签 NDA（2-4 周）；(3) 部署测试环境、导入对方数据（1 周）；(4) 收集反馈、整理需求（持续）。保守估计会占用 0.5 人力持续 2-3 个月。在 4.5 人的小团队里，0.5 人就是 11% 的产能。

2. 内部 dogfooding 的价值在于"快速反馈循环"。团队自己导入真实数据（可以从 App Store 评论、公开论坛抓取示例数据，或使用团队自己的项目反馈），自己使用搜索和标签功能，发现 AI 管线的 bug 和不足，当天修复。这种"开发-使用-修复"的循环周期是小时级的，而 Design Partner 的反馈循环是周级的。

3. 路线图中 Phase 3 的验收标准（`07-roadmap.md`）写的是"通过 CSV 导入 1000 条用户反馈，系统自动完成四阶段 AI 处理"和"通过 `vector_search` 搜索'支付卡顿'，返回语义相关的结果"——这些验收标准完全可以由团队内部完成，不需要外部用户参与。

**最大风险**：团队成员对产品的使用模式可能与真实目标用户（VOC 分析师、产品经理）完全不同。我们可能优化了"工程师视角下的搜索体验"，却忽略了产品经理真正需要的"按时间维度筛选 + 批量导出 + 生成报告"等工作流。dogfooding 验证的是技术可行性，不是产品市场匹配度。

---

### 王磊 · g 第一阶段是否需要包含治理/采纳机制？

**立场**：反对（先出产品再补机制）

**一句话观点**：4.5 人团队没有余力做"非代码"工作，治理机制留到 Phase 4 再说。

**关键论据**：

1. 方若琳在团队定义（`00-expert-team.md`）中描述的治理闭环是"数据进去 → AI 处理 → 洞察产出 → 用户行动 → 行动结果反馈回系统"。这是一个 Phase 4 级别的完整闭环（路线图 Phase 4 的核心交付物包含 Signal → Concept 两阶段知识资产管理、人机共治工作台、五个治理操作）。在第一阶段引入任何形式的治理机制，都是在提前做 Phase 4 的工作，违反了路线图"渐进式能力叠加"的原则。

2. 工时估算：即使是"最小治理机制"——"标记有用/无用"按钮，也需要：
   - 后端：反馈记录 API（1 人天）+ 反馈存储 schema（0.5 人天）+ 标签合并 API（2 人天）+ 合并逻辑（处理引用关系迁移等，2 人天）= 5.5 人天
   - 前端：标签旁边加反馈按钮（1 人天）+ 合并操作 UI（2 人天）+ 反馈列表页面（1 人天）= 4 人天
   - 合计约 9.5 人天。这几乎等于前端全部 Must Have 的工时。

3. 路线图的退出策略（`09-resource-roi.md` 第 5.3 节）明确说"Phase 3 后"的已获得资产是"核心产品已形成——可以开始试用、收集反馈、产生业务价值"。注意它说的是"开始"收集反馈——意味着 Phase 3 的定位就是"最小可用产品"，治理机制属于 Phase 4 的增量能力。我们应该忠实执行路线图的设计意图，不要在 Phase 3 里塞 Phase 4 的东西。

**最大风险**：没有任何反馈机制意味着 AI 管线的错误会默默积累——错误的标签不会被纠正，膨胀的标签不会被合并。如果 dogfooding 阶段积累了 5000 条数据后才引入治理机制，可能面对一个需要大量人工清洗的"脏标签库"。但我认为这是可接受的——Phase 4 引入治理机制后，可以对历史数据做一次批量重处理。

---

*本文档由虚拟专家团队模拟器生成，两位专家独立撰写，观点未经协调。*
